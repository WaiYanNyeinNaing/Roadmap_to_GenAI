# -*- coding: utf-8 -*-
"""Text_generation_GPT2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X1F_kF4PgAN1fvH5xRKtaYG4e3KLy1Qb
"""

!pip install transformers

"""**Model Description: GPT-2 (smallest version) Model**

| **Aspect**             | **Description** |
|------------------------|-----------------|
| **Name**               | GPT-2 (smallest) (OpenAI)  |
| **Architecture**       | Transformer-based model with 124 million parameters. |
| **Training Data**      | WebText dataset from 45 million website links, diverse internet text. |
| **Key Features**       | - Layer normalization at each sub-block's input. <br> - Residual layer weights scaling. <br> - Expanded vocabulary (50,257 tokens). <br> - Increased context size (1024 tokens). <br> - Larger batch size (512). |
| **Capabilities**       | Text generation, translation, summarization, question answering without task-specific training. |
| **Model Card**         | [Hugging Face GPT-2 Model Card](https://huggingface.co/gpt2) |
"""

from transformers import AutoModelForCausalLM, AutoTokenizer

# Replace "gpt2" with the your custom downloaded model name
model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

from transformers import pipeline

# Load the pipeline
text_generator = pipeline("text-generation", model="gpt2")
# Generate text with a prompt
generated_text = text_generator("In a world dominated by AI,")
# Print the generated text
print(generated_text)



"""**Text completion models like GPT-2 are versatile and can be used for a variety of tasks beyond simple text generation. Here are a few examples, along with code snippets using the pipeline approach:**"""

from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer

# Load the model and tokenizer
model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# Load the pipeline
text_generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

"""**1. Story Generation**

Generate a story based on a given prompt.
"""

prompt = "Once upon a time in Burma (Myanmar),"
story = text_generator(prompt, max_length=200)[0]['generated_text']
print(" ")
print(story)

""" **2. Dialogue Generation**

Simulate a conversation by providing a line of dialogue.
"""

prompt = "Human: Hello, how are you today?\nAI:"
dialogue = text_generator(prompt, max_length=50)[0]['generated_text']
print(" ")
print(dialogue)

"""**3. Writing Assistance**

Complete an email or a letter.
"""

prompt = "Dear Wai Yan, I am writing to you regarding Machine Learning course"
email = text_generator(prompt, max_length=100)[0]['generated_text']
print(" ")
print(email)

"""**4. Poetry Generation**

Create poetry from a given line or theme
"""

prompt = "The wind whispered through the trees,"
poem = text_generator(prompt, max_length=50, temperature=1.5)[0]['generated_text']
print(" ")
print(poem)

"""**5. Question Answering**

Answer questions based on context provided.
"""

prompt = "Q: What is the capital of France?\n A:"
answer = text_generator(prompt, max_length=30,temperature=0.1)[0]['generated_text']
print(" ")
print(answer)

"""**6. Ideas Brainstorming**

Generate ideas on a given topic.
"""

#prompt = "List innovative start-up ideas that combine Artificial Intelligence technology with Automobile: 1. "
prompt = "Come up with five concepts for mobile apps that help improve mental health and wellbeing: 1."


ideas = text_generator(prompt, max_length=100,temperature=1)[0]['generated_text']
print(" ")
print(ideas)

"""**7. Philosophical Musings**

Generate philosophical thoughts or musings.
"""

prompt = "What is the meaning of life . .?"
philosophy = text_generator(prompt, max_length=100)[0]['generated_text']
print(philosophy)



"""**Advance Prompt :** Add more hyper paramter to control the model output"""

from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer

# Load the model and tokenizer
model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# Load the pipeline
text_generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# Generate text with a prompt and additional hyperparameters
generated_text = text_generator(
    "In a world dominated by AI, human",
    max_length=100,           # Maximum length of the output text
    num_return_sequences=1,  # Number of sequences to generate
    temperature=0.7,         # Controls randomness. Lower is less random.
    top_k=50,                # Keeps the top k most likely next words at each step
    top_p=0.95,              # Nucleus sampling: keeps the minimum number of words with cumulative probability > top_p
    repetition_penalty=1.2,  # Penalty for repeated words
    do_sample=True           # Whether to sample the next token or take the most likely one
)

# Print the generated text
for output in generated_text:
    print(" ")
    print(output['generated_text'])